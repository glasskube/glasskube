---
slug: kubeflow-pt2
title: I don't know what to write yet
description: Something something Kubeflow.
authors: [ christophenne ]
tags: [ kubeflow ]
image: https://github.com/user-attachments/assets/37ecf483-a984-4ca5-b10f-eaad367ff7cd
---

*Another great guide to some technologies by someone who has no idea lets goooooo.*

<!-- truncate -->

We tried to create a label predictor for new GitHub issues in our [glasskube/glasskube](https://github.com/glasskube/glasskube) repo.
It doesn't really work. Turns out 300 issues aren't enough data, plus ML skills would have been beneficiary. Anyway here is what we did.

## Setup

This requires a working [Kubeflow installation](/).

As we are far from being ML experts, we based our experiments on these excellent resources:
* [How to Fine-Tune LLMs with Kubeflow](https://www.kubeflow.org/docs/components/training/user-guides/fine-tuning/)
* [Fine-Tune BERT LLM for Sentiment Analysis with Kubeflow PyTorchJob (Jupyter Notebook)](https://github.com/kubeflow/training-operator/blob/master/examples/pytorch/text-classification/Fine-Tune-BERT-LLM.ipynb)
* [Fine-tuning BERT (and friends) for multi-label text classification](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb)

We combined these to perform our LLM finetuning for multi-label classification with kubeflow, with the following adaptions:
* We use our own dataset containing the github issues of our repo.
* Our custom dataset will be loaded from a GCS bucket instead of huggingface, and we publish the finetuned model back to a GCS bucket.
* Finally, we serve the finetuned model with KServe.

## Preparing the dataset

We use the Github CLI `gh` to generate the dataset:

```shell
gh issue list --state all --limit 1000 \
  --json title,body,labels \
  --jq 'map(del(.labels) + (.labels | map( { (.name|tostring): true } ) | add))' \
  > prepared-issues.json
```

The `--jq` part maps the string labels to be true-valued booleans in each JSON object. This works for any Github repo.
It might make sense to take only a subset of labels into consideration, like in our case `enhancement` and `bug`, e.g. like this:

```shell
jq 'map({"title": .title, "body": .body, "enhancement": (.enhancement // false), "bug": (.bug // false)})' prepared-issues.json > prepared-issues-reduced.json
```

Consequently a GitHub issue will be represented the following way:

```json
{
  "title": "Support transitive dependencies",
  "body": "..."
  "enhancement": true,
  "bug": false
}
```

A bucket inside the Google cloud storage of your project can be used to store this prepared dataset, from where it can be accessed by the training code afterwards.

## Distributed training across GPU nodes

Our custom dataset will be loaded from a GCS bucket instead of huggingface, and we publish the finetuned model back to a GCS bucket,
from where it will be available for inference via KServe.

## Automated finetuning with Kubeflow Pipelines

With [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/overview/), tasks like the previously shown finetuning can be automated without the user having to manually execute commands via the notebook.
This might be the preferred way once the training part has been figured out. After all, one might want to run finetuning again with an updated dataset in the future.

After making ourselves familiar with the concept of KFP and running a first [hello world pipeline](https://www.kubeflow.org/docs/components/pipelines/getting-started/)
we migrated the previously shown Jupyter Notebook to a Kubeflow pipeline with one component.

```python
from kfp import dsl, compiler

@dsl.component(packages_to_install=[
    "gcsfs",
    "transformers",
    "datasets==2.16",
    "evaluate==0.4.3",
    "accelerate",
    "scikit-learn",
    "kubeflow-training"
])
def start_distributed_training(
    bucket: str,
    dataset_file: str,
    output_model_name: str,
    gproject: str) -> str:

    # imports ...

    def train_func(parameters):
        # imports again ...
        # train_func like in the notebook

    job_name = "training-pipeline-job"
    # Create PyTorchJob
    # ...
    # Wait until PyTorchJob has Running condition.
    job = TrainingClient().wait_for_job_conditions(
        job_name,
        expected_conditions={"Running"},
    )
    return "job is running"

@dsl.pipeline
def training_pipeline(
    bucket: str,
    dataset_file: str,
    output_model_name: str,
    gproject: str
) -> str:
    training_task = start_distributed_training(
        bucket=bucket,
        dataset_file=dataset_file,
        output_model_name=output_model_name,
        gproject=gproject
    )
    return training_task.output

compiler.Compiler().compile(training_pipeline, 'training_pipeline.yaml')
```

Running this script will compile the pipeline into the `training_pipeline.yaml` file, which can be uploaded and used in Kubeflow.
There might be better ways to deal with this in a more integrated manner.

The pipeline can be started in the Kubeflow UI, which is the point at which the parameters have to be defined. In our case these are the GCS bucket name,
the filename of the dataset, the name of the finetuned model, and the ID of the google cloud project. These are passed from the UI down
to the Pipeline, and from there on to the component and the training function.

## Serving the finetuned model

## Conclusion

At least it kind of runs.
